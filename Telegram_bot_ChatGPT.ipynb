{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexTako/telegram_GPT_bot/blob/main/Telegram_bot_ChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание телеграм-бота, который будет отвечать на вопросы из моей базы знаний. Для создания телеграм-бота использовал библиотеку aiogram3. При отправке Telegram-боту команды `/help` он возвращает информацию о базе знаний: тематика, число записей в базе знаний, пример запроса к базе.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c6WZSsChNoIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваше решение\n",
        "# Отключим предупреждения в колабе. Будет меньше лишней информации в выводе\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "sJ36fUZpMlPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai mwclient mwparserfromhell tiktoken"
      ],
      "metadata": {
        "id": "ysmwX9EwEUQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f6da93a-7005-4d4d-b17d-c53b1d10af32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
            "Requirement already satisfied: mwclient in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: mwparserfromhell in /usr/local/lib/python3.10/dist-packages (0.6.6)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.5.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from mwclient) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from mwclient) (1.16.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib->mwclient) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import mwclient  # библиотека для работы с MediaWiki API для загрузки примеров статей Википедии\n",
        "import mwparserfromhell  # Парсер для MediaWiki\n",
        "import openai  # будем использовать для токинизации\n",
        "import pandas as pd  # В DataFrame будем хранить базу знаний и результат токинизации базы знаний\n",
        "import re  # для вырезания ссылок <ref> из статей Википедии\n",
        "import tiktoken  # для подсчета токенов"
      ],
      "metadata": {
        "id": "5NPcyrfTEX1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# поиск страниц Википедии об истории России\n",
        "\n",
        "# Задаем категорию и рускоязычную версию Википедии для поиска\n",
        "CATEGORY_TITLE = \"Category:история России\"\n",
        "WIKI_SITE = \"ru.wikipedia.org\"\n",
        "\n",
        "# Соберем заголовки всех статей\n",
        "def titles_from_category(\n",
        "    category: mwclient.listing.Category, # Задаем типизированный параметр категории статей\n",
        "    max_depth: int # Определяем глубину вложения статей\n",
        ") -> set[str]:\n",
        "    \"\"\"Возвращает набор заголовков страниц в данной категории Википедии и ее подкатегориях.\"\"\"\n",
        "    titles = set() # Используем множество для хранения заголовков статей\n",
        "    for cm in category.members(): # Перебираем вложенные объекты категории\n",
        "        if type(cm) == mwclient.page.Page: # Если объект является страницей\n",
        "            titles.add(cm.name) # в хранилище заголовков добавляем имя страницы\n",
        "        elif isinstance(cm, mwclient.listing.Category) and max_depth > 0: # Если объект является категорией и глубина вложения не достигла максимальной\n",
        "            deeper_titles = titles_from_category(cm, max_depth=max_depth - 1) # вызываем рекурсивно функцию для подкатегории\n",
        "            titles.update(deeper_titles) # добавление в множество элементов из другого множества\n",
        "    return titles\n",
        "\n",
        "# Инициализация объекта MediaWiki\n",
        "# WIKI_SITE ссылается на русскоязычную часть Википедии\n",
        "site = mwclient.Site(WIKI_SITE)\n",
        "\n",
        "# Загрузка раздела заданной категории\n",
        "category_page = site.pages[CATEGORY_TITLE]\n",
        "# Получение множества всех заголовков категории с вложенностью на один уровень\n",
        "titles = titles_from_category(category_page, max_depth=1)\n",
        "\n",
        "\n",
        "print(f\"Создано {len(titles)} заголовков статей в категории {CATEGORY_TITLE}.\")"
      ],
      "metadata": {
        "id": "Gn8uf8WyEbJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f82c4ce8-d3ed-4473-de3e-2708410b7029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Создано 757 заголовков статей в категории Category:история России.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Задаем секции, которые будут отброшены при парсинге статей\n",
        "SECTIONS_TO_IGNORE = [\n",
        "    \"See also\",\n",
        "    \"References\",\n",
        "    \"External links\",\n",
        "    \"Further reading\",\n",
        "    \"Footnotes\",\n",
        "    \"Bibliography\",\n",
        "    \"Sources\",\n",
        "    \"Citations\",\n",
        "    \"Literature\",\n",
        "    \"Footnotes\",\n",
        "    \"Notes and references\",\n",
        "    \"Photo gallery\",\n",
        "    \"Works cited\",\n",
        "    \"Photos\",\n",
        "    \"Gallery\",\n",
        "    \"Notes\",\n",
        "    \"References and sources\",\n",
        "    \"References and notes\",\n",
        "]"
      ],
      "metadata": {
        "id": "FJTQZWi-FMjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция возвращает список всех вложенных секций для заданной секции страницы Википедии\n",
        "\n",
        "def all_subsections_from_section(\n",
        "    section: mwparserfromhell.wikicode.Wikicode, # текущая секция\n",
        "    parent_titles: list[str], # Заголовки родителя\n",
        "    sections_to_ignore: set[str], # Секции, которые необходимо проигнорировать\n",
        ") -> list[tuple[list[str], str]]:\n",
        "    \"\"\"\n",
        "    Из раздела Википедии возвращает список всех вложенных секций.\n",
        "    Каждый подраздел представляет собой кортеж, где:\n",
        "      - первый элемент представляет собой список родительских секций, начиная с заголовка страницы\n",
        "      - второй элемент представляет собой текст секции\n",
        "    \"\"\"\n",
        "\n",
        "    # Извлекаем заголовки текущей секции\n",
        "    headings = [str(h) for h in section.filter_headings()]\n",
        "    title = headings[0]\n",
        "    # Заголовки Википедии имеют вид: \"== Heading ==\"\n",
        "\n",
        "    if title.strip(\"=\" + \" \") in sections_to_ignore:\n",
        "        # Если заголовок секции в списке для игнора, то пропускаем его\n",
        "        return []\n",
        "\n",
        "    # Объединим заголовки и подзаголовки, чтобы сохранить контекст для chatGPT\n",
        "    titles = parent_titles + [title]\n",
        "\n",
        "    # Преобразуем wikicode секции в строку\n",
        "    full_text = str(section)\n",
        "\n",
        "    # Выделяем текст секции без заголовка\n",
        "    section_text = full_text.split(title)[1]\n",
        "    if len(headings) == 1:\n",
        "        # Если один заголовок, то формируем результирующий список\n",
        "        return [(titles, section_text)]\n",
        "    else:\n",
        "        first_subtitle = headings[1]\n",
        "        section_text = section_text.split(first_subtitle)[0]\n",
        "        # Формируем результирующий список из текста до первого подзаголовка\n",
        "        results = [(titles, section_text)]\n",
        "        for subsection in section.get_sections(levels=[len(titles) + 1]):\n",
        "            results.extend(\n",
        "                # Вызываем функцию получения вложенных секций для заданной секции\n",
        "                all_subsections_from_section(subsection, titles, sections_to_ignore)\n",
        "                )  # Объединяем результирующие списки данной функции и вызываемой\n",
        "        return results\n",
        "\n",
        "# Функция возвращает список всех секций страницы, за исключением тех, которые отбрасываем\n",
        "def all_subsections_from_title(\n",
        "    title: str, # Заголовок статьи Википедии, которую парсим\n",
        "    sections_to_ignore: set[str] = SECTIONS_TO_IGNORE, # Секции, которые игнорируем\n",
        "    site_name: str = WIKI_SITE, # Ссылка на сайт википедии\n",
        ") -> list[tuple[list[str], str]]:\n",
        "    \"\"\"\n",
        "    Из заголовка страницы Википедии возвращает список всех вложенных секций.\n",
        "    Каждый подраздел представляет собой кортеж, где:\n",
        "      - первый элемент представляет собой список родительских секций, начиная с заголовка страницы\n",
        "      - второй элемент представляет собой текст секции\n",
        "    \"\"\"\n",
        "\n",
        "    # Инициализация объекта MediaWiki\n",
        "    # WIKI_SITE ссылается на русскоязычную часть Википедии\n",
        "    site = mwclient.Site(site_name)\n",
        "\n",
        "    # Запрашиваем страницу по заголовку\n",
        "    page = site.pages[title]\n",
        "\n",
        "    # Получаем текстовое представление страницы\n",
        "    text = page.text()\n",
        "\n",
        "    # Удобный парсер для MediaWiki\n",
        "    parsed_text = mwparserfromhell.parse(text)\n",
        "    # Извлекаем заголовки\n",
        "    headings = [str(h) for h in parsed_text.filter_headings()]\n",
        "    if headings: # Если заголовки найдены\n",
        "        # В качестве резюме берем текст до первого заголовка\n",
        "        summary_text = str(parsed_text).split(headings[0])[0]\n",
        "    else:\n",
        "        # Если нет заголовков, то весь текст считаем резюме\n",
        "        summary_text = str(parsed_text)\n",
        "    results = [([title], summary_text)] # Добавляем резюме в результирующий список\n",
        "    for subsection in parsed_text.get_sections(levels=[2]): # Извлекаем секции 2-го уровня\n",
        "        results.extend(\n",
        "            # Вызываем функцию получения вложенных секций для заданной секции\n",
        "            all_subsections_from_section(subsection, [title], sections_to_ignore)\n",
        "        ) # Объединяем результирующие списки данной функции и вызываемой\n",
        "    return results"
      ],
      "metadata": {
        "id": "c6wG9zLhJm_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Разбивка статей на секции\n",
        "# придется немного подождать, так как на парсинг 100 статей требуется около минуты\n",
        "wikipedia_sections = []\n",
        "for title in titles:\n",
        "    wikipedia_sections.extend(all_subsections_from_title(title))\n",
        "print(f\"Найдено {len(wikipedia_sections)} секций на {len(titles)} страницах\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGRt1DWCKc5N",
        "outputId": "c9a97179-e356-417e-88d4-78ef2abe5bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Найдено 4988 секций на 757 страницах\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Очистка текста секции от ссылок <ref>xyz</ref>, начальных и конечных пробелов\n",
        "def clean_section(section: tuple[list[str], str]) -> tuple[list[str], str]:\n",
        "    titles, text = section\n",
        "    # Удаляем ссылки\n",
        "    text = re.sub(r\"<ref.*?</ref>\", \"\", text)\n",
        "    # Удаляем пробелы вначале и конце\n",
        "    text = text.strip()\n",
        "    return (titles, text)\n",
        "\n",
        "# Применим функцию очистки ко всем секциям с помощью генератора списков\n",
        "wikipedia_sections = [clean_section(ws) for ws in wikipedia_sections]\n",
        "\n",
        "# Отфильтруем короткие и пустые секции\n",
        "def keep_section(section: tuple[list[str], str]) -> bool:\n",
        "    \"\"\"Возвращает значение True, если раздел должен быть сохранен, в противном случае значение False.\"\"\"\n",
        "    titles, text = section\n",
        "    # Фильтруем по произвольной длине, можно выбрать и другое значение\n",
        "    if len(text) < 16:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "\n",
        "original_num_sections = len(wikipedia_sections)\n",
        "wikipedia_sections = [ws for ws in wikipedia_sections if keep_section(ws)]\n",
        "print(f\"Отфильтровано {original_num_sections-len(wikipedia_sections)} секций, осталось {len(wikipedia_sections)} секций.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLfEs6PzLIK6",
        "outputId": "162c292b-06cf-46e4-d0c4-1e9d53ca62e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Отфильтровано 514 секций, осталось 4474 секций.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_MODEL = \"gpt-3.5-turbo\"  # only matters insofar as it selects which tokenizer to use\n",
        "\n",
        "# Функция подсчета токенов\n",
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Возвращает число токенов в строке.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "# Функция разделения строк\n",
        "def halved_by_delimiter(string: str, delimiter: str = \"\\n\") -> list[str, str]:\n",
        "    \"\"\"Разделяет строку надвое с помощью разделителя (delimiter), пытаясь сбалансировать токены с каждой стороны.\"\"\"\n",
        "\n",
        "    # Делим строку на части по разделителю, по умолчанию \\n - перенос строки\n",
        "    chunks = string.split(delimiter)\n",
        "    if len(chunks) == 1:\n",
        "        return [string, \"\"]  # разделитель не найден\n",
        "    elif len(chunks) == 2:\n",
        "        return chunks  # нет необходимости искать промежуточную точку\n",
        "    else:\n",
        "        # Считаем токены\n",
        "        total_tokens = num_tokens(string)\n",
        "        halfway = total_tokens // 2\n",
        "        # Предварительное разделение по середине числа токенов\n",
        "        best_diff = halfway\n",
        "        # В цикле ищем какой из разделителей, будет ближе всего к best_diff\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            left = delimiter.join(chunks[: i + 1])\n",
        "            left_tokens = num_tokens(left)\n",
        "            diff = abs(halfway - left_tokens)\n",
        "            if diff >= best_diff:\n",
        "                break\n",
        "            else:\n",
        "                best_diff = diff\n",
        "        left = delimiter.join(chunks[:i])\n",
        "        right = delimiter.join(chunks[i:])\n",
        "        # Возвращаем левую и правую часть оптимально разделенной строки\n",
        "        return [left, right]\n",
        "\n",
        "\n",
        "# Функция обрезает строку до максимально разрешенного числа токенов\n",
        "def truncated_string(\n",
        "    string: str, # строка\n",
        "    model: str, # модель\n",
        "    max_tokens: int, # максимальное число разрешенных токенов\n",
        "    print_warning: bool = True, # флаг вывода предупреждения\n",
        ") -> str:\n",
        "    \"\"\"Обрезка строки до максимально разрешенного числа токенов.\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    encoded_string = encoding.encode(string)\n",
        "    # Обрезаем строку и декодируем обратно\n",
        "    truncated_string = encoding.decode(encoded_string[:max_tokens])\n",
        "    if print_warning and len(encoded_string) > max_tokens:\n",
        "        print(f\"Предупреждение: Строка обрезана с {len(encoded_string)} токенов до {max_tokens} токенов.\")\n",
        "    # Усеченная строка\n",
        "    return truncated_string\n",
        "\n",
        "# Функция делит секции статьи на части по максимальному числу токенов\n",
        "def split_strings_from_subsection(\n",
        "    subsection: tuple[list[str], str], # секции\n",
        "    max_tokens: int = 1000, # максимальное число токенов\n",
        "    model: str = GPT_MODEL, # модель\n",
        "    max_recursion: int = 5, # максимальное число рекурсий\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Разделяет секции на список из частей секций, в каждой части не более max_tokens.\n",
        "    Каждая часть представляет собой кортеж родительских заголовков [H1, H2, ...] и текста (str).\n",
        "    \"\"\"\n",
        "    titles, text = subsection\n",
        "    string = \"\\n\\n\".join(titles + [text])\n",
        "    num_tokens_in_string = num_tokens(string)\n",
        "    # Если длина соответствует допустимой, то вернет строку\n",
        "    if num_tokens_in_string <= max_tokens:\n",
        "        return [string]\n",
        "    # если в результате рекурсия не удалось разделить строку, то просто усечем ее по числу токенов\n",
        "    elif max_recursion == 0:\n",
        "        return [truncated_string(string, model=model, max_tokens=max_tokens)]\n",
        "    # иначе разделим пополам и выполним рекурсию\n",
        "    else:\n",
        "        titles, text = subsection\n",
        "        for delimiter in [\"\\n\\n\", \"\\n\", \". \"]: # Пробуем использовать разделители от большего к меньшему (разрыв, абзац, точка)\n",
        "            left, right = halved_by_delimiter(text, delimiter=delimiter)\n",
        "            if left == \"\" or right == \"\":\n",
        "                # если какая-либо половина пуста, повторяем попытку с более простым разделителем\n",
        "                continue\n",
        "            else:\n",
        "                # применим рекурсию на каждой половине\n",
        "                results = []\n",
        "                for half in [left, right]:\n",
        "                    half_subsection = (titles, half)\n",
        "                    half_strings = split_strings_from_subsection(\n",
        "                        half_subsection,\n",
        "                        max_tokens=max_tokens,\n",
        "                        model=model,\n",
        "                        max_recursion=max_recursion - 1, # уменьшаем максимальное число рекурсий\n",
        "                    )\n",
        "                    results.extend(half_strings)\n",
        "                return results\n",
        "    # иначе никакого разделения найдено не было, поэтому просто обрезаем строку (должно быть очень редко)\n",
        "    return [truncated_string(string, model=model, max_tokens=max_tokens)]\n"
      ],
      "metadata": {
        "id": "gh3o74H5MSnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Делим секции на части\n",
        "MAX_TOKENS = 1600\n",
        "wikipedia_strings = []\n",
        "for section in wikipedia_sections:\n",
        "    wikipedia_strings.extend(split_strings_from_subsection(section, max_tokens=MAX_TOKENS))\n",
        "\n",
        "print(f\"{len(wikipedia_sections)} секций Википедии поделены на {len(wikipedia_strings)} строк.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1eK_yopN4FX",
        "outputId": "236faa18-62be-44f7-f4d9-ea802044b897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предупреждение: Строка обрезана с 2209 токенов до 1600 токенов.\n",
            "Предупреждение: Строка обрезана с 1788 токенов до 1600 токенов.\n",
            "Предупреждение: Строка обрезана с 1774 токенов до 1600 токенов.\n",
            "Предупреждение: Строка обрезана с 1885 токенов до 1600 токенов.\n",
            "Предупреждение: Строка обрезана с 1788 токенов до 1600 токенов.\n",
            "Предупреждение: Строка обрезана с 2944 токенов до 1600 токенов.\n",
            "Предупреждение: Строка обрезана с 1655 токенов до 1600 токенов.\n",
            "Предупреждение: Строка обрезана с 1768 токенов до 1600 токенов.\n",
            "Предупреждение: Строка обрезана с 3119 токенов до 1600 токенов.\n",
            "Предупреждение: Строка обрезана с 2276 токенов до 1600 токенов.\n",
            "Предупреждение: Строка обрезана с 2294 токенов до 1600 токенов.\n",
            "Предупреждение: Строка обрезана с 1609 токенов до 1600 токенов.\n",
            "Предупреждение: Строка обрезана с 1783 токенов до 1600 токенов.\n",
            "4474 секций Википедии поделены на 5677 строк.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # Модель токенизации от OpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Введите OpenAI API Key:\")\n",
        "client = OpenAI(api_key = os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Функция отправки chatGPT строки для ее токенизации (вычисления эмбедингов)\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "\n",
        "   return client.embeddings.create(input = [text], model=model).data[0].embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuA3LcMrOPBp",
        "outputId": "2eacdfbc-97cd-4881-f385-9545dfe0f1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"text\": wikipedia_strings})\n",
        "\n",
        "df['embedding'] = df.text.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
        "\n",
        "SAVE_PATH = \"./russia_history.csv\"\n",
        "# Сохранение результата\n",
        "df.to_csv(SAVE_PATH, index=False)"
      ],
      "metadata": {
        "id": "daDP8xoYOWml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "embeddings_path = \"./russia_history.csv\"\n",
        "\n",
        "df = pd.read_csv(embeddings_path)\n",
        "\n",
        "# Конвертируем наши эмбединги из строк в списки\n",
        "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
      ],
      "metadata": {
        "id": "forZyehXdbkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial  # вычисляет сходство векторов\n",
        "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
        "\n",
        "# Функция поиска\n",
        "def strings_ranked_by_relatedness(\n",
        "    query: str, # пользовательский запрос\n",
        "    df: pd.DataFrame, # DataFrame со столбцами text и embedding (база знаний)\n",
        "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y), # функция схожести, косинусное расстояние\n",
        "    top_n: int = 100 # выбор лучших n-результатов\n",
        ") -> tuple[list[str], list[float]]: # Функция возвращает кортеж двух списков, первый содержит строки, второй - числа с плавающей запятой\n",
        "    \"\"\"Возвращает строки и схожести, отсортированные от большего к меньшему\"\"\"\n",
        "\n",
        "    # Отправляем в OpenAI API пользовательский запрос для токенизации\n",
        "    query_embedding_response = openai.embeddings.create(\n",
        "        model=EMBEDDING_MODEL,\n",
        "        input=query,\n",
        "    )\n",
        "\n",
        "    # Получен токенизированный пользовательский запрос\n",
        "    query_embedding = query_embedding_response.data[0].embedding\n",
        "\n",
        "    # Сравниваем пользовательский запрос с каждой токенизированной строкой DataFrame\n",
        "    strings_and_relatednesses = [\n",
        "        (row[\"text\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
        "        for i, row in df.iterrows()\n",
        "    ]\n",
        "\n",
        "    # Сортируем по убыванию схожести полученный список\n",
        "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Преобразовываем наш список в кортеж из списков\n",
        "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
        "\n",
        "    # Возвращаем n лучших результатов\n",
        "    return strings[:top_n], relatednesses[:top_n]"
      ],
      "metadata": {
        "id": "-P1L3PSiPrmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
        "    \"\"\"Возвращает число токенов в строке для заданной модели\"\"\"\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "    return len(encoding.encode(text))\n",
        "\n",
        "# Функция формирования запроса к chatGPT по пользовательскому вопросу и базе знаний\n",
        "def query_message(\n",
        "    query: str, # пользовательский запрос\n",
        "    df: pd.DataFrame, # DataFrame со столбцами text и embedding (база знаний)\n",
        "    model: str, # модель\n",
        "    token_budget: int # ограничение на число отсылаемых токенов в модель\n",
        ") -> str:\n",
        "    \"\"\"Возвращает сообщение для GPT с соответствующими исходными текстами, извлеченными из фрейма данных (базы знаний).\"\"\"\n",
        "    strings, relatednesses = strings_ranked_by_relatedness(query, df) # функция ранжирования базы знаний по пользовательскому запросу\n",
        "    # Шаблон инструкции для chatGPT\n",
        "    message = 'Используйте приведенные ниже статьи по истории России, чтобы ответить на следующий вопрос. Если ответ в статьях не будет найден, напиши \"Извините, я не знаю ответа на этот вопрос\"'\n",
        "    # Шаблон для вопроса\n",
        "    question = f\"\\n\\nQuestion: {query}\"\n",
        "\n",
        "    # Добавляем к сообщению для chatGPT релевантные строки из базы знаний, пока не выйдем за допустимое число токенов\n",
        "    for string in strings:\n",
        "        next_article = f'\\n\\nWikipedia article section:\\n\"\"\"\\n{string}\\n\"\"\"'\n",
        "        if (num_tokens(message + next_article + question, model=model) > token_budget):\n",
        "            break\n",
        "        else:\n",
        "            message += next_article\n",
        "    return message + question\n",
        "\n",
        "\n",
        "def ask(\n",
        "    query: str, # пользовательский запрос\n",
        "    df: pd.DataFrame = df, # DataFrame со столбцами text и embedding (база знаний)\n",
        "    model: str = GPT_MODEL, # модель\n",
        "    token_budget: int = 4096 - 500, # ограничение на число отсылаемых токенов в модель\n",
        "    print_message: bool = False, # нужно ли выводить сообщение перед отправкой\n",
        ") -> str:\n",
        "    \"\"\"Отвечает на вопрос, используя GPT и базу знаний.\"\"\"\n",
        "    # Формируем сообщение к chatGPT (функция выше)\n",
        "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
        "    # Если параметр True, то выводим сообщение\n",
        "    if print_message:\n",
        "        print(message)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Ты отвечаешь на вопрос по истории России\"},\n",
        "        {\"role\": \"user\", \"content\": message},\n",
        "    ]\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0 # гиперпараметр степени случайности при генерации текста. Влияет на то, как модель выбирает следующее слово в последовательности.\n",
        "    )\n",
        "    response_message = response.choices[0].message.content\n",
        "    return response_message"
      ],
      "metadata": {
        "id": "PFRHDABLP3w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dbMyrTbQPuq",
        "outputId": "eadc3dab-5bde-4ff9-aefc-e5418c5e4e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiogram aiosqlite"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwCWvsgtTnN1",
        "outputId": "6f61b95a-acf7-464c-d8e3-1e241338af73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiogram in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.10/dist-packages (0.20.0)\n",
            "Requirement already satisfied: aiofiles~=23.2.1 in /usr/local/lib/python3.10/dist-packages (from aiogram) (23.2.1)\n",
            "Requirement already satisfied: aiohttp~=3.9.0 in /usr/local/lib/python3.10/dist-packages (from aiogram) (3.9.3)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from aiogram) (2024.2.2)\n",
            "Requirement already satisfied: magic-filter<1.1,>=1.0.12 in /usr/local/lib/python3.10/dist-packages (from aiogram) (1.0.12)\n",
            "Requirement already satisfied: pydantic<2.6,>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from aiogram) (2.5.3)\n",
            "Requirement already satisfied: typing-extensions<=5.0,>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from aiogram) (4.9.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp~=3.9.0->aiogram) (4.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.6,>=2.4.1->aiogram) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.6,>=2.4.1->aiogram) (2.14.6)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.0->aiogram) (3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import aiosqlite\n",
        "import asyncio\n",
        "import logging\n",
        "from aiogram import Bot, Dispatcher, types\n",
        "from aiogram.filters.command import Command\n",
        "from aiogram.utils.keyboard import InlineKeyboardBuilder, ReplyKeyboardBuilder\n",
        "from aiogram import F"
      ],
      "metadata": {
        "id": "JSOL5vDUTlzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level= logging.INFO)\n",
        "os.environ[\"API_KEY\"] = getpass.getpass(\"Введите Telegram API Key:\")\n",
        "\n",
        "# Объект класса бот\n",
        "bot = Bot(token= os.environ.get('API_KEY'))\n",
        "\n",
        "# Диспетчер\n",
        "dp = Dispatcher()\n",
        "\n",
        "# Хэндлер на /start\n",
        "@dp.message(Command('start'))\n",
        "async def cmd_start(message: types.Message):\n",
        "    builder = ReplyKeyboardBuilder()\n",
        "    await message.answer('Привет! Я бот для поиска информации по истории России, пожалуйста, введите Ваш запрос. Также вы можете ввести команду /help, чтобы получить информацию о базе знаний')\n",
        "\n",
        "@dp.message(Command('help'))\n",
        "async def help_me(message: types.Message): # Будет выводить следующую информацию: тематика, число записей в базе знаний, пример запроса к базе.\n",
        "    await message.answer(f'Тематика бота: История России')\n",
        "    await message.answer(f'Число записей в базе знаний: {df.shape[0]}')\n",
        "    await message.answer(f'Пример запроса: Когда было крещение Руси?')\n",
        "\n",
        "@dp.message()\n",
        "async def ask_question(message: types.Message):\n",
        "    await message.answer(ask(message))\n",
        "\n",
        "# Запуск процесса поллинга новых апдейтов\n",
        "async def main():\n",
        "\n",
        "    await dp.start_polling(bot)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bji_DSwuR8_l",
        "outputId": "21b0cb43-672c-4e62-a541-9e9ece539842"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите OpenAI API Key:··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:aiogram.dispatcher:Received SIGINT signal\n"
          ]
        }
      ]
    }
  ]
}